# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""
Endpoint and Process dataclasses for worker topology.

This module replaces the bash array math in Jinja templates with typed Python:

Before (bash):
    for i in $(seq 0 $((PREFILL_WORKERS - 1))); do
        leader_idx=$((WORKER_NODE_OFFSET + i * PREFILL_NODES_PER_WORKER))
        prefill_leaders[$i]=$leader_idx
    done

After (Python):
    endpoints = allocate_endpoints(config, nodes)
    for endpoint in endpoints:
        print(f"{endpoint.mode} worker {endpoint.index} on {endpoint.nodes}")
"""

from collections.abc import Sequence
from dataclasses import dataclass, field
from typing import Literal

# Worker mode type
WorkerMode = Literal["prefill", "decode", "agg"]


@dataclass(frozen=True)
class Endpoint:
    """A logical worker endpoint (serving unit).

    An endpoint represents one logical worker that may span multiple nodes.
    For example, a prefill worker with TP=16 on a cluster with 8 GPUs/node
    would span 2 nodes.

    Attributes:
        mode: Worker mode ("prefill", "decode", or "agg")
        index: Zero-based index within the mode (e.g., prefill worker 0, 1, 2)
        nodes: Tuple of node hostnames this endpoint uses
        gpu_indices: Set of GPU indices used on each node (e.g., {0,1,2,3,4,5,6,7})
        gpus_per_node: Number of GPUs per node in the cluster
    """

    mode: WorkerMode
    index: int
    nodes: tuple[str, ...]
    gpu_indices: frozenset[int] = field(default_factory=lambda: frozenset(range(8)))
    gpus_per_node: int = 8

    @property
    def leader_node(self) -> str:
        """The first node in the endpoint (used for distributed init)."""
        return self.nodes[0]

    @property
    def num_nodes(self) -> int:
        """Number of nodes this endpoint spans."""
        return len(self.nodes)

    @property
    def total_gpus(self) -> int:
        """Total GPUs used by this endpoint across all nodes."""
        return self.num_nodes * len(self.gpu_indices)

    @property
    def is_multi_node(self) -> bool:
        """Whether this endpoint spans multiple nodes."""
        return self.num_nodes > 1


@dataclass(frozen=True)
class Process:
    """A physical process within an endpoint.

    For most backends, there's one Process per node within an Endpoint.
    This dataclass holds the per-process configuration needed for srun.

    Attributes:
        node: The node hostname this process runs on
        gpu_indices: GPU indices visible to this process
        sys_port: DYN_SYSTEM_PORT for this process
        endpoint_mode: The mode of the parent endpoint
        endpoint_index: The index of the parent endpoint
        node_rank: Rank within the endpoint (0 for leader)
    """

    node: str
    gpu_indices: frozenset[int]
    sys_port: int
    endpoint_mode: WorkerMode
    endpoint_index: int
    node_rank: int = 0

    @property
    def is_leader(self) -> bool:
        """Whether this is the leader process for the endpoint."""
        return self.node_rank == 0

    @property
    def cuda_visible_devices(self) -> str:
        """CUDA_VISIBLE_DEVICES string for this process."""
        return ",".join(str(i) for i in sorted(self.gpu_indices))


def allocate_endpoints(
    num_prefill: int,
    num_decode: int,
    num_agg: int,
    gpus_per_prefill: int,
    gpus_per_decode: int,
    gpus_per_agg: int,
    gpus_per_node: int,
    available_nodes: Sequence[str],
) -> list[Endpoint]:
    """Allocate endpoints to nodes based on GPU requirements.

    This is the core allocation logic that replaces bash array math.

    Args:
        num_prefill: Number of prefill workers
        num_decode: Number of decode workers
        num_agg: Number of aggregated workers
        gpus_per_prefill: GPUs per prefill worker
        gpus_per_decode: GPUs per decode worker
        gpus_per_agg: GPUs per agg worker
        gpus_per_node: GPUs available per node
        available_nodes: List of available node hostnames

    Returns:
        List of Endpoint objects with node assignments

    Example:
        # 2 prefill workers with 8 GPUs each, 4 decode workers with 4 GPUs each
        # on 4 nodes with 8 GPUs/node
        endpoints = allocate_endpoints(
            num_prefill=2, num_decode=4, num_agg=0,
            gpus_per_prefill=8, gpus_per_decode=4, gpus_per_agg=0,
            gpus_per_node=8, available_nodes=["node1", "node2", "node3", "node4"]
        )
        # Results:
        # - prefill_0 on node1 (8 GPUs)
        # - prefill_1 on node2 (8 GPUs)
        # - decode_0 on node3 (GPUs 0-3)
        # - decode_1 on node3 (GPUs 4-7)
        # - decode_2 on node4 (GPUs 0-3)
        # - decode_3 on node4 (GPUs 4-7)
    """
    endpoints: list[Endpoint] = []
    node_idx = 0
    gpu_offset = 0  # Track GPU offset within current node

    def allocate_worker(mode: WorkerMode, index: int, gpus_needed: int) -> Endpoint:
        """Allocate a single worker endpoint."""
        nonlocal node_idx, gpu_offset

        if gpus_needed <= 0:
            raise ValueError(f"gpus_needed must be positive, got {gpus_needed}")

        # Calculate how many nodes this worker spans
        nodes_per_worker = (gpus_needed + gpus_per_node - 1) // gpus_per_node

        # For multi-node workers, start fresh on node boundary
        if (nodes_per_worker > 1 or gpus_needed == gpus_per_node) and gpu_offset > 0:
            node_idx += 1
            gpu_offset = 0

        # Collect nodes for this worker
        worker_nodes = []
        for _ in range(nodes_per_worker):
            if node_idx >= len(available_nodes):
                raise ValueError(f"Not enough nodes: need node {node_idx}, but only {len(available_nodes)} available")
            worker_nodes.append(available_nodes[node_idx])
            node_idx += 1

        # Determine GPU indices (full node for multi-node, or specific range for single)
        if nodes_per_worker > 1:
            gpu_indices = frozenset(range(gpus_per_node))
            gpu_offset = 0
        else:
            # Single node: might be partial or full
            if gpu_offset + gpus_needed > gpus_per_node:
                # Doesn't fit, move to next node
                node_idx += 1
                if node_idx > len(available_nodes):
                    raise ValueError("Not enough nodes for GPU allocation")
                worker_nodes = [available_nodes[node_idx - 1]]
                gpu_offset = 0

            gpu_indices = frozenset(range(gpu_offset, gpu_offset + gpus_needed))
            gpu_offset += gpus_needed

            # If we filled the node, move to next
            if gpu_offset >= gpus_per_node:
                node_idx += 1
                gpu_offset = 0
            else:
                # Still on same node, rewind node_idx for next partial worker
                node_idx -= len(worker_nodes) - 1 if len(worker_nodes) > 1 else 0

        # Fix: for single-node workers staying on same node
        if nodes_per_worker == 1 and gpu_offset > 0 and gpu_offset < gpus_per_node:
            # We're still on the same node, don't increment
            pass
        elif nodes_per_worker == 1 and gpu_offset == 0:
            # We moved to a new node after filling previous
            pass

        return Endpoint(
            mode=mode,
            index=index,
            nodes=tuple(worker_nodes),
            gpu_indices=gpu_indices,
            gpus_per_node=gpus_per_node,
        )

    # Reset for cleaner allocation
    node_idx = 0
    gpu_offset = 0

    # Simpler allocation: each worker gets nodes sequentially
    def allocate_workers_simple(mode: WorkerMode, count: int, gpus_per_worker: int) -> list[Endpoint]:
        nonlocal node_idx, gpu_offset
        result = []

        nodes_per_worker = (gpus_per_worker + gpus_per_node - 1) // gpus_per_node

        for i in range(count):
            if nodes_per_worker >= 1 and gpus_per_worker >= gpus_per_node:
                # Multi-node or full-node worker
                worker_nodes = tuple(available_nodes[node_idx + j] for j in range(nodes_per_worker))
                node_idx += nodes_per_worker

                result.append(
                    Endpoint(
                        mode=mode,
                        index=i,
                        nodes=worker_nodes,
                        gpu_indices=frozenset(range(gpus_per_node)),
                        gpus_per_node=gpus_per_node,
                    )
                )
            else:
                # Partial node worker - pack multiple on same node
                if gpu_offset + gpus_per_worker > gpus_per_node:
                    node_idx += 1
                    gpu_offset = 0

                worker_node = available_nodes[node_idx]
                gpu_indices = frozenset(range(gpu_offset, gpu_offset + gpus_per_worker))
                gpu_offset += gpus_per_worker

                if gpu_offset >= gpus_per_node:
                    node_idx += 1
                    gpu_offset = 0

                result.append(
                    Endpoint(
                        mode=mode,
                        index=i,
                        nodes=(worker_node,),
                        gpu_indices=gpu_indices,
                        gpus_per_node=gpus_per_node,
                    )
                )

        return result

    # Allocate in order: prefill, decode, agg
    if num_prefill > 0:
        endpoints.extend(allocate_workers_simple("prefill", num_prefill, gpus_per_prefill))

    if num_decode > 0:
        endpoints.extend(allocate_workers_simple("decode", num_decode, gpus_per_decode))

    if num_agg > 0:
        endpoints.extend(allocate_workers_simple("agg", num_agg, gpus_per_agg))

    return endpoints


def endpoints_to_processes(
    endpoints: list[Endpoint],
    base_port: int = 8081,
) -> list[Process]:
    """Convert endpoints to physical processes.

    For SGLang, we create one process per node in each endpoint.
    Each process gets a unique DYN_SYSTEM_PORT.

    Args:
        endpoints: List of Endpoint objects
        base_port: Starting port for DYN_SYSTEM_PORT assignment

    Returns:
        List of Process objects
    """
    processes: list[Process] = []
    current_port = base_port

    for endpoint in endpoints:
        for node_rank, node in enumerate(endpoint.nodes):
            processes.append(
                Process(
                    node=node,
                    gpu_indices=endpoint.gpu_indices,
                    sys_port=current_port,
                    endpoint_mode=endpoint.mode,
                    endpoint_index=endpoint.index,
                    node_rank=node_rank,
                )
            )
            current_port += 1

    return processes
