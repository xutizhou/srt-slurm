#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --nodes={{ total_nodes }}
{% if backend_type == "trtllm" %}
#SBATCH --ntasks={{ gpus_per_node * total_nodes }}
#SBATCH --ntasks-per-node={{ gpus_per_node }}
{% else %}
#SBATCH --ntasks={{ total_nodes }}
#SBATCH --ntasks-per-node=1
{% endif %}
{% if use_gpus_per_node_directive %}
#SBATCH --gpus-per-node={{ gpus_per_node }}
{% endif %}
{% if use_segment_sbatch_directive %}
#SBATCH --segment={{ total_nodes }}
{% endif %}
{% if use_exclusive_sbatch_directive %}
#SBATCH --exclusive
{% endif %}
#SBATCH --account={{ account }}
#SBATCH --time={{ time_limit }}
#SBATCH --output={{ output_base }}/%j/logs/sweep_%j.log
#SBATCH --partition={{ partition }}
{% for key, value in sbatch_directives.items() %}
{% if value %}
#SBATCH --{{ key }}={{ value }}
{% else %}
#SBATCH --{{ key }}
{% endif %}
{% endfor %}

# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# Orchestrator runs on HOST (needs srun access), spawns containerized workers
# Config: {{ config_path }}
# Generated: {{ timestamp }}

set -e

# Setup directories
SRTCTL_SOURCE="{{ srtctl_source }}"
OUTPUT_BASE="{{ output_base }}"
OUTPUT_DIR="${OUTPUT_BASE}/${SLURM_JOB_ID}"
LOG_DIR="${OUTPUT_DIR}/logs"
mkdir -p "${LOG_DIR}"

# Export for Python orchestrator to use the same paths
export SRTCTL_OUTPUT_DIR="${OUTPUT_DIR}"

# Redirect stderr to stdout (SLURM captures both via --output)
exec 2>&1

echo "=========================================="
echo "ðŸš€ srtctl Sweep Orchestrator"
echo "=========================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Config: {{ config_path }}"
echo "Nodes: ${SLURM_JOB_NUM_NODES}"
echo "Container: {{ container_image }}"
echo "Start: $(date)"
echo "=========================================="
echo ""

# Copy config to output directory
cp "{{ config_path }}" "${OUTPUT_DIR}/config.yaml"

# Get head node
HEAD_NODE=$(scontrol show hostnames ${SLURM_NODELIST} | head -n1)
echo "Head node: ${HEAD_NODE}"

# Install srtctl using container's pip (host may not have pip)
SRTCTL_INSTALL_DIR="${OUTPUT_DIR}/.srtctl_install"
mkdir -p "${SRTCTL_INSTALL_DIR}"
export SRTCTL_SOURCE_DIR="${SRTCTL_SOURCE}"

echo ""
echo "Installing srtctl dependencies..."

# Install uv if not present (single binary, no dependencies)
if ! command -v uv &> /dev/null; then
    echo "Installing uv package manager..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.local/bin:$PATH"
fi

# Use uv to install srtctl (handles native extensions properly)
echo "Using uv to install srtctl..."
uv pip install --quiet --target="${SRTCTL_INSTALL_DIR}" "${SRTCTL_SOURCE}"

export PYTHONPATH="${SRTCTL_INSTALL_DIR}:${PYTHONPATH}"
echo "Installed to ${SRTCTL_INSTALL_DIR}"

{% if setup_script %}
# Custom setup script override from CLI
export SRTCTL_SETUP_SCRIPT="{{ setup_script }}"
{% endif %}

echo "Running orchestrator on host (with srun access)..."
echo ""

# Run orchestrator on the HOST (not in container) so it has access to srun
# The orchestrator will spawn containerized workers
# Output goes to SLURM's --output (sweep_%j.log)
set +e  # Temporarily disable exit-on-error to capture exit code
python3 -u -m srtctl.cli.do_sweep "{{ config_path }}"
EXIT_CODE=$?
set -e  # Re-enable exit-on-error

echo ""
echo "=========================================="
if [ $EXIT_CODE -eq 0 ]; then
    echo "âœ“ Sweep completed successfully"
else
    echo "âœ— Sweep failed (exit code: $EXIT_CODE)"
fi
echo "End: $(date)"
echo "=========================================="

exit ${EXIT_CODE}