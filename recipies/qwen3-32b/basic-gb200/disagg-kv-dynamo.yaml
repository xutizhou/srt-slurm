# Qwen3-32B Disaggregated KV-Aware Routing Configuration
# Based on dynamo exemplar for Mooncake benchmark
# Configuration: 6 prefill + 2 decode workers with TP2 on 16x H200 GPUs (2 nodes)

name: "disagg-kv-dynamo"

model:
  path: "qwen3-32b"
  container: "lmsysorg/sglang:v0.5.8-runtime"
  precision: "bf16"

resources:
  gpu_type: "gb200"
  gpus_per_node: 4
  # Disaggregated mode: 6 prefill + 2 decode
  # 6 prefill workers @ TP2 = 12 GPUs (1.5 nodes)
  # 2 decode workers @ TP2 = 4 GPUs (0.5 nodes)
  # Total: 16 GPUs across 2 nodes
  prefill_nodes: 3
  decode_nodes: 1
  prefill_workers: 6
  decode_workers: 2

# until prefill router goes in 0.8.0
dynamo:
  version: "0.8.0"

frontend:
  type: dynamo
  enable_multiple_frontends: false
  args:
    router-mode: "kv"
    router-reset-states: true

backend:
  type: sglang

  prefill_environment:
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    DYN_REQUEST_PLANE: "nats"

  decode_environment:
    DYN_REQUEST_PLANE: "nats"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"

  sglang_config:
    prefill:
      served-model-name: "Qwen/Qwen3-32B"
      trust-remote-code: true
      tensor-parallel-size: 2
      disaggregation-mode: "prefill"
      mem-fraction-static: 0.90
      # Rope scaling for extended context
      json-model-override-args: '{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768},"max_position_embeddings":131072}'
      context-length: 131072
      page-size: 64
      disaggregation-transfer-backend: "nixl"

    decode:
      served-model-name: "Qwen/Qwen3-32B"
      trust-remote-code: true
      tensor-parallel-size: 2
      disaggregation-mode: "decode"
      mem-fraction-static: 0.90
      # Rope scaling for extended context
      json-model-override-args: '{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768},"max_position_embeddings":131072}'
      context-length: 131072
      page-size: 64
      disaggregation-transfer-backend: "nixl"

# Mooncake benchmark using aiperf
benchmark:
  type: "mooncake-router"
  mooncake_workload: "conversation"
  ttft_threshold_ms: 2000
  itl_threshold_ms: 25
