name: ctx1_gen1_dep8_batch768_eplb0_mtp0_6144

model:
  path: "dsr1-fp8"
  container: "dynamo-trtllm"
  precision: "fp8"

resources:
  gpu_type: "gb200"
  prefill_nodes: 2
  prefill_workers: 1
  gpus_per_prefill: 8

  decode_workers: 1
  decode_nodes: 2
  gpus_per_decode: 8

  gpus_per_node: 4

backend:
  type: trtllm

  prefill_environment:
    TLLM_LOG_LEVEL: "INFO"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    TRTLLM_ENABLE_PDL: "1"
    ENROOT_ALLOW_DEV: "yes"
    NCCL_GRAPH_MIXING_SUPPORT: "0"

  decode_environment:
    TLLM_LOG_LEVEL: "INFO"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    TRTLLM_ENABLE_PDL: "1"
    ENROOT_ALLOW_DEV: "yes"
    NCCL_GRAPH_MIXING_SUPPORT: "0"
    TRTLLM_FORCE_COMM_METHOD: "NVLINK_TWO_SIDED"
    ENABLE_CONFIGURABLE_MOE: "1"

  trtllm_config:
    prefill:
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 16384
      cuda_graph_config: null
      disable_overlap_scheduler: true
      enable_attention_dp: true
      kv_cache_config:
        dtype: fp8
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.5
      max_batch_size: 16
      max_num_tokens: 16384
      max_seq_len: 1064
      moe_config:
        backend: DEEPGEMM
      moe_expert_parallel_size: 8
      pipeline_parallel_size: 1
      print_iter_log: true
      tensor_parallel_size: 8


    decode:
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 16384
      cuda_graph_config:
        batch_sizes:
        - 1
        - 2
        - 4
        - 8
        - 16
        - 24
        - 32
        - 40
        - 48
        - 56
        - 64
        - 72
        - 80
        - 88
        - 96
        - 104
        - 112
        - 120
        - 128
        - 136
        - 144
        - 152
        - 160
        - 168
        - 176
        - 184
        - 192
        - 200
        - 208
        - 216
        - 224
        - 232
        - 240
        - 248
        - 256
        - 264
        - 272
        - 280
        - 288
        - 296
        - 304
        - 312
        - 320
        - 328
        - 336
        - 344
        - 352
        - 360
        - 368
        - 376
        - 384
        - 392
        - 400
        - 408
        - 416
        - 424
        - 432
        - 440
        - 448
        - 456
        - 464
        - 472
        - 480
        - 488
        - 496
        - 504
        - 512
        - 520
        - 528
        - 536
        - 544
        - 552
        - 560
        - 568
        - 576
        - 584
        - 592
        - 600
        - 608
        - 616
        - 624
        - 632
        - 640
        - 648
        - 656
        - 664
        - 672
        - 680
        - 688
        - 696
        - 704
        - 712
        - 720
        - 728
        - 736
        - 744
        - 752
        - 760
        - 768
        enable_padding: true
      enable_attention_dp: true
      enable_lm_head_tp_in_adp: false
      kv_cache_config:
        dtype: fp8
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.8
      max_batch_size: 768
      max_num_tokens: 768
      max_seq_len: 2088
      moe_config:
        backend: DEEPGEMM
        use_low_precision_moe_combine: true
      moe_expert_parallel_size: 8
      num_postprocess_workers: 4
      pipeline_parallel_size: 1
      print_iter_log: true
      stream_interval: 100
      tensor_parallel_size: 8


benchmark:
  type: "sa-bench"
  isl: 1024
  osl: 1024
  concurrencies: ['6144']
  req_rate: "inf"

frontend:
  type: "dynamo"
  nginx_container: "nginx-sqsh"


health_check:
  max_attempts: 360
  interval_seconds: 10

dynamo:
  install: false
infra:
  etcd_nats_dedicated_node: true
