#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --nodes={{ total_nodes }}
#SBATCH --ntasks={{ total_nodes }}
{% if use_segment_sbatch_directive %}
#SBATCH --segment={{ total_nodes }}
{% endif %}
#SBATCH --ntasks-per-node=1
{% if use_gpus_per_node_directive %}
#SBATCH --gpus-per-node={{ gpus_per_node }}
{% endif %}
#SBATCH --account={{ account }}
#SBATCH --time={{ time_limit }}
#SBATCH --output={{ log_dir_prefix }}/%j_{{ prefill_workers }}P_{{ decode_workers }}D_{{ timestamp }}/log.out
#SBATCH --partition={{ partition }}

# Constants
set -x
PREFILL_NODES={{ prefill_nodes }}
DECODE_NODES={{ decode_nodes }}
PREFILL_WORKERS={{ prefill_workers }}
DECODE_WORKERS={{ decode_workers }}
TOTAL_NODES=$((PREFILL_NODES + DECODE_NODES))
GPUS_PER_NODE={{ gpus_per_node }}
TOTAL_GPUS=$((TOTAL_NODES * GPUS_PER_NODE))
PREFILL_GPUS=$((PREFILL_NODES * GPUS_PER_NODE))
DECODE_GPUS=$((DECODE_NODES * GPUS_PER_NODE))
PREFILL_NODES_PER_WORKER=$((PREFILL_NODES / PREFILL_WORKERS))
DECODE_NODES_PER_WORKER=$((DECODE_NODES / DECODE_WORKERS))
{% if log_dir_prefix.startswith('/') %}
LOG_DIR="{{ log_dir_prefix }}/${SLURM_JOB_ID}_{{ prefill_workers }}P_{{ decode_workers }}D_{{ timestamp }}"
{% else %}
LOG_DIR="${SLURM_SUBMIT_DIR}/{{ log_dir_prefix }}/${SLURM_JOB_ID}_{{ prefill_workers }}P_{{ decode_workers }}D_{{ timestamp }}"
{% endif %}
SCRIPT_DIR="${SLURM_SUBMIT_DIR}/scripts"
OUTPUT_DIR="${SLURM_SUBMIT_DIR}/outputs"
MODEL_DIR="{{ model_dir }}"
CONFIG_DIR="{{ config_dir }}"
CONTAINER_IMAGE="{{ container_image }}"
NETWORK_INTERFACE="{{ network_interface }}"
GPU_TYPE="{{ gpu_type | default('h100') }}"
set +x

{% raw %}

mkdir -p "${OUTPUT_DIR}" "${LOG_DIR}"

# Source utility functions for robust IP discovery
source "${SCRIPT_DIR}/utils/slurm_utils.sh"

nodes=($(scontrol show hostnames $SLURM_NODELIST))
if [ ${#nodes[@]} -ne $TOTAL_NODES ]; then
    echo "Error: Expected $TOTAL_NODES nodes but got ${#nodes[@]} nodes"
    exit 1
fi

# Print node information
for i in "${!nodes[@]}"; do
    echo "Node $i: ${nodes[$i]}"
done

{% endraw %}
{% if enable_multiple_frontends and not use_sglang_router %}
{% raw %}
# Multiple frontend architecture
# Node 0: nginx only + prefill shard
# Node 1: NATS/ETCD + first frontend + prefill shard
# Node 2+: prefill/decode workers + optional additional frontends

NGINX_NODE=${nodes[0]}
MASTER_NODE=${nodes[1]}
MASTER_IP=$(get_node_ip "$MASTER_NODE" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
if [ -z "$MASTER_IP" ]; then
    echo "Error: Could not retrieve IP address for master host $MASTER_NODE"
    exit 1
fi
echo "Master IP address (node 1): $MASTER_IP"
echo "Nginx node (node 0): $NGINX_NODE"

# Generate frontend IP list for nginx config
frontend_hosts=()
frontend_ips=()
# Node 1 always has a frontend (with NATS/ETCD)
frontend_hosts+=("$MASTER_NODE")
frontend_ips+=("$MASTER_IP")

# Add additional frontends based on num_additional_frontends
{% endraw %}ADDITIONAL_FRONTENDS={{ num_additional_frontends }}{% raw %}
if [ "$ADDITIONAL_FRONTENDS" -gt 0 ]; then
    # Calculate which nodes get additional frontends
    # We have TOTAL_NODES prefill/decode nodes, distribute additional frontends across them
    nodes_per_frontend=$(( (TOTAL_NODES - 1 + ADDITIONAL_FRONTENDS - 1) / ADDITIONAL_FRONTENDS ))  # ceil division
    frontend_node_idx=2  # Start from node 2 (node 1 already has frontend)

    for i in $(seq 1 $ADDITIONAL_FRONTENDS); do
        if [ $frontend_node_idx -lt $TOTAL_NODES ]; then
            node_name=${nodes[$frontend_node_idx]}
            node_ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
            frontend_hosts+=("$node_name")
            frontend_ips+=("$node_ip")
            echo "Additional frontend $i on node $frontend_node_idx: $node_name ($node_ip)"
            frontend_node_idx=$((frontend_node_idx + nodes_per_frontend))
        fi
    done
fi

echo "Frontend hosts: ${frontend_hosts[@]}"
echo "Frontend IPs: ${frontend_ips[@]}"

# Generate nginx configuration
# Build a Python list literal of frontend hosts from the bash array
FRONTEND_LIST=$(printf "'%s'," "${frontend_ips[@]}")
FRONTEND_LIST="[${FRONTEND_LIST%,}]"
export FRONTEND_LIST SCRIPT_DIR LOG_DIR
python3 - <<'PY'
import os
from jinja2 import Template

template_path = os.path.join(os.environ['SCRIPT_DIR'], 'templates/nginx.conf.j2')
output_path = os.path.join(os.environ['LOG_DIR'], 'nginx.conf')

with open(template_path, 'r') as f:
    tmpl = Template(f.read())

frontend_hosts = eval(os.environ['FRONTEND_LIST'])
config = tmpl.render(frontend_hosts=frontend_hosts)

with open(output_path, 'w') as f:
    f.write(config)
PY

{% endraw %}
{% else %}
{% raw %}
# Traditional architecture - first prefill node handles everything
MASTER_IP=$(get_node_ip "${nodes[0]}" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
if [ -z "$MASTER_IP" ]; then
    echo "Error: Could not retrieve IP address for master host ${nodes[0]}"
    exit 1
fi
echo "Master IP address: $MASTER_IP"
{% endraw %}
{% endif %}
{% raw %}

# Compute leader nodes for each worker
{% endraw %}
{% if enable_multiple_frontends and not use_sglang_router %}
{% raw %}
# With multiple frontends: keep offset 0; nginx coexists on node 0
WORKER_NODE_OFFSET=0
{% endraw %}
{% else %}
{% raw %}
# Traditional: workers start from node 0
WORKER_NODE_OFFSET=0
{% endraw %}
{% endif %}
{% raw %}

prefill_leaders=()
for i in $(seq 0 $((PREFILL_WORKERS - 1))); do
    leader_idx=$((WORKER_NODE_OFFSET + i * PREFILL_NODES_PER_WORKER))
    prefill_leaders[$i]=$leader_idx
done

decode_leaders=()
for i in $(seq 0 $((DECODE_WORKERS - 1))); do
    leader_idx=$((WORKER_NODE_OFFSET + PREFILL_NODES + i * DECODE_NODES_PER_WORKER))
    decode_leaders[$i]=$leader_idx
done

echo "Prefill worker leaders: ${prefill_leaders[@]}"
echo "Decode worker leaders: ${decode_leaders[@]}"

# Prepare enroot arguments to pass to srun commands
ENROOT_ARGS="\
    --container-image=${CONTAINER_IMAGE} \
    --no-container-entrypoint \
    --no-container-mount-home \
    --container-mounts=${MODEL_DIR}:/model/,${CONFIG_DIR}:/configs/,${SCRIPT_DIR}:/scripts/,${OUTPUT_DIR}:/outputs/,${LOG_DIR}:/logs/{% endraw %}{% if extra_container_mounts %},{{ extra_container_mounts }}{% endif %}{% raw %} \
"
{% endraw %}

# Build common worker arguments
{% raw %}
WORKER_ARGS="--gpu_type ${GPU_TYPE} --gpus_per_node ${GPUS_PER_NODE} --master_ip ${MASTER_IP}"
{% endraw %}
{% if use_sglang_router %}
{% raw %}
WORKER_ARGS="$WORKER_ARGS --use-sglang-router"
{% endraw %}
{% endif %}
{% if enable_multiple_frontends and not use_sglang_router %}
{% raw %}
# Add multiple frontends flag for worker setup
WORKER_ARGS="$WORKER_ARGS --multiple-frontends-enabled"
{% endraw %}
{% endif %}
{% raw %}
# Set profiler mode from config
WORKER_ARGS="$WORKER_ARGS --profiler {% endraw %}{{ profiler }}{% raw %}"
{% endraw %}
{% raw %}
# Add SGLang config path (mounted in container at /logs/)
WORKER_ARGS="$WORKER_ARGS --sglang-config-path /logs/sglang_config.yaml"
{% endraw %}
{% if setup_script %}
# Add custom setup script if provided
WORKER_ARGS="$WORKER_ARGS --setup-script {{ setup_script }}"
{% endif %}
{% raw %}

{% endraw %}
{% if enable_multiple_frontends and not use_sglang_router %}
{% raw %}
{% endraw %}
{% if total_nodes > 1 %}
{% raw %}
# Launch nginx on node 0
echo "Launching nginx on ${NGINX_NODE}"
cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$NGINX_NODE --output=${LOG_DIR}/${NGINX_NODE}_nginx.out python /scripts/worker_setup.py --worker_type nginx --nginx_config /logs/nginx.conf ${WORKER_ARGS}"
echo "$cmd"
$cmd &
{% endraw %}
{% endif %}
{% raw %}

# Launch frontend on master node (node 1) - this will also start NATS/ETCD
echo "Launching frontend + NATS/ETCD on master node ${MASTER_NODE}"
cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$MASTER_NODE --output=${LOG_DIR}/${MASTER_NODE}_frontend_0.out python /scripts/worker_setup.py --worker_type frontend --worker_idx 0 ${WORKER_ARGS}"
echo "$cmd"
$cmd &

# Launch additional frontends on designated nodes
if [ "$ADDITIONAL_FRONTENDS" -gt 0 ]; then
    frontend_idx=1  # Start from 1 since node 1 is frontend 0
    nodes_per_frontend=$(( (TOTAL_NODES - 2 + ADDITIONAL_FRONTENDS - 1) / ADDITIONAL_FRONTENDS ))
    frontend_node_idx=2

    for i in $(seq 1 $ADDITIONAL_FRONTENDS); do
        if [ $frontend_node_idx -lt $TOTAL_NODES ]; then
            node=${nodes[$frontend_node_idx]}
            echo "Launching additional frontend $frontend_idx on node $frontend_node_idx: $node"
            cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$node --output=${LOG_DIR}/${node}_frontend_${frontend_idx}.out python /scripts/worker_setup.py --worker_type frontend --worker_idx ${frontend_idx} ${WORKER_ARGS}"
            echo "$cmd"
            $cmd &
            frontend_idx=$((frontend_idx + 1))
            frontend_node_idx=$((frontend_node_idx + nodes_per_frontend))
        fi
    done
fi
{% endraw %}
{% endif %}
{% raw %}

# Launch prefill workers
for worker_idx in $(seq 0 $((PREFILL_WORKERS - 1))); do
    leader_idx=${prefill_leaders[$worker_idx]}
    leader_node=${nodes[$leader_idx]}

    # Get leader IP for this worker group
    LEADER_IP=$(get_node_ip "$leader_node" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    echo "Prefill worker $worker_idx leader: $leader_node ($LEADER_IP)"

    # Launch all nodes for this worker
    for node_idx in $(seq 0 $((PREFILL_NODES_PER_WORKER - 1))); do
        global_node_idx=$((leader_idx + node_idx))
        node=${nodes[$global_node_idx]}
        local_rank=$node_idx

        echo "Launching prefill worker $worker_idx, node $global_node_idx (local_rank $local_rank): $node"
{% endraw %}
{% if enable_config_dump %}
{% raw %}
        CONFIG_DUMP_ARG="--dump-config-path /logs/${node}_config.json"
{% endraw %}
{% else %}
{% raw %}
        CONFIG_DUMP_ARG=""
{% endraw %}
{% endif %}
{% raw %}
        cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$node --output=${LOG_DIR}/${node}_prefill_w${worker_idx}.out python /scripts/worker_setup.py --leader_ip ${LEADER_IP} --worker_idx ${worker_idx} --local_rank ${local_rank} --nodes_per_worker ${PREFILL_NODES_PER_WORKER} --worker_type prefill ${WORKER_ARGS} ${CONFIG_DUMP_ARG}"
        echo "$cmd"
        $cmd &
    done
done

# Launch decode workers
for worker_idx in $(seq 0 $((DECODE_WORKERS - 1))); do
    leader_idx=${decode_leaders[$worker_idx]}
    leader_node=${nodes[$leader_idx]}

    # Get leader IP for this worker group
    LEADER_IP=$(get_node_ip "$leader_node" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    echo "Decode worker $worker_idx leader: $leader_node ($LEADER_IP)"

    # Launch all nodes for this worker
    for node_idx in $(seq 0 $((DECODE_NODES_PER_WORKER - 1))); do
        global_node_idx=$((leader_idx + node_idx))
        node=${nodes[$global_node_idx]}
        local_rank=$node_idx

        echo "Launching decode worker $worker_idx, node $global_node_idx (local_rank $local_rank): $node"
{% endraw %}
{% if enable_config_dump %}
{% raw %}
        CONFIG_DUMP_ARG="--dump-config-path /logs/${node}_config.json"
{% endraw %}
{% else %}
{% raw %}
        CONFIG_DUMP_ARG=""
{% endraw %}
{% endif %}
{% raw %}
        cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$node --output=${LOG_DIR}/${node}_decode_w${worker_idx}.out python /scripts/worker_setup.py --leader_ip ${LEADER_IP} --worker_idx ${worker_idx} --local_rank ${local_rank} --nodes_per_worker ${DECODE_NODES_PER_WORKER} --worker_type decode ${CONFIG_DUMP_ARG} ${WORKER_ARGS}"
        echo "$cmd"
        $cmd &
    done
done

echo ""
{% endraw %}
{% if enable_multiple_frontends and not use_sglang_router %}
{% raw %}
echo "Frontend available at: http://${NGINX_NODE}:8000"
echo "To connect to the nginx node:"
echo "srun $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${NGINX_NODE} --overlap --pty bash"
echo "To connect to the master node (NATS/ETCD):"
echo "srun $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${MASTER_NODE} --overlap --pty bash"
{% endraw %}
{% else %}
{% raw %}
echo "To connect to the host prefill node:"
echo "srun $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${nodes[0]} --overlap --pty bash"
{% endraw %}
{% endif %}
{% raw %}

# Launch sglang router(s) when enabled
{% endraw %}{% if use_sglang_router %}{% raw %}
# Collect leader IPs for prefill and decode
PREFILL_LEADER_IPS=()
for idx in "${prefill_leaders[@]}"; do
    node_name=${nodes[$idx]}
    ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    PREFILL_LEADER_IPS+=("$ip")
done
DECODE_LEADER_IPS=()
for idx in "${decode_leaders[@]}"; do
    node_name=${nodes[$idx]}
    ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    DECODE_LEADER_IPS+=("$ip")
done

PREFILL_IPS_STR=$(IFS=,; echo "${PREFILL_LEADER_IPS[*]}")
DECODE_IPS_STR=$(IFS=,; echo "${DECODE_LEADER_IPS[*]}")

{% endraw %}
{% if enable_multiple_frontends %}
{% raw %}
# Multiple router architecture (mirrors dynamo frontend scaling)
# Node 0: nginx load balancer + first router
# Node 1+: additional routers distributed across worker nodes

NGINX_NODE=${nodes[0]}
NGINX_IP=$(get_node_ip "$NGINX_NODE" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")

# Build router host/IP lists
router_hosts=()
router_ips=()

# First router always on node 0
router_hosts+=("$NGINX_NODE")
router_ips+=("$NGINX_IP")

# Add additional routers (uses same num_additional_frontends setting as dynamo)
{% endraw %}ADDITIONAL_ROUTERS={{ num_additional_frontends }}{% raw %}
if [ "$ADDITIONAL_ROUTERS" -gt 0 ]; then
    # Calculate which nodes get additional routers
    # Distribute additional routers across nodes, starting from node 1
    nodes_per_router=$(( (TOTAL_NODES - 1 + ADDITIONAL_ROUTERS - 1) / ADDITIONAL_ROUTERS ))  # ceil division
    router_node_idx=1  # Start from node 1 (node 0 already has first router)

    for i in $(seq 1 $ADDITIONAL_ROUTERS); do
        if [ $router_node_idx -lt $TOTAL_NODES ]; then
            node_name=${nodes[$router_node_idx]}
            node_ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
            router_hosts+=("$node_name")
            router_ips+=("$node_ip")
            echo "Additional router $i on node $router_node_idx: $node_name ($node_ip)"
            router_node_idx=$((router_node_idx + nodes_per_router))
        fi
    done
fi

echo "Router hosts: ${router_hosts[@]}"
echo "Router IPs: ${router_ips[@]}"

# Generate nginx configuration for router load balancing
# Routers use internal port 30080, nginx exposes on 8000
ROUTER_INTERNAL_PORT=30080
ROUTER_LIST=$(printf "'%s'," "${router_ips[@]}")
ROUTER_LIST="[${ROUTER_LIST%,}]"
export ROUTER_LIST ROUTER_INTERNAL_PORT SCRIPT_DIR LOG_DIR
python3 - <<'PY'
import os
from jinja2 import Template

template_path = os.path.join(os.environ['SCRIPT_DIR'], 'templates/nginx.conf.j2')
output_path = os.path.join(os.environ['LOG_DIR'], 'nginx.conf')

with open(template_path, 'r') as f:
    tmpl = Template(f.read())

router_hosts = eval(os.environ['ROUTER_LIST'])
backend_port = int(os.environ['ROUTER_INTERNAL_PORT'])
config = tmpl.render(frontend_hosts=router_hosts, backend_port=backend_port)

with open(output_path, 'w') as f:
    f.write(config)
PY

# Launch nginx on node 0
echo "Launching nginx for router load balancing on ${NGINX_NODE}"
cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$NGINX_NODE --output=${LOG_DIR}/${NGINX_NODE}_nginx.out python /scripts/worker_setup.py --worker_type nginx --nginx_config /logs/nginx.conf ${WORKER_ARGS}"
echo "$cmd"
$cmd &

# Launch first router on node 0 (with nginx)
# Router listens on internal port, nginx proxies from 8000
echo "Launching sglang-router 0 on ${NGINX_NODE} (internal port ${ROUTER_INTERNAL_PORT})"
cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$NGINX_NODE --output=${LOG_DIR}/${NGINX_NODE}_router_0.out python /scripts/worker_setup.py --worker_type sglang-router --worker_idx 0 --prefill-ips ${PREFILL_IPS_STR} --decode-ips ${DECODE_IPS_STR} --router-port ${ROUTER_INTERNAL_PORT} ${WORKER_ARGS}"
echo "$cmd"
$cmd &

# Launch additional routers on designated nodes
if [ "$ADDITIONAL_ROUTERS" -gt 0 ]; then
    router_idx=1
    nodes_per_router=$(( (TOTAL_NODES - 1 + ADDITIONAL_ROUTERS - 1) / ADDITIONAL_ROUTERS ))
    router_node_idx=1

    for i in $(seq 1 $ADDITIONAL_ROUTERS); do
        if [ $router_node_idx -lt $TOTAL_NODES ]; then
            node=${nodes[$router_node_idx]}
            echo "Launching sglang-router $router_idx on node $router_node_idx: $node (internal port ${ROUTER_INTERNAL_PORT})"
            cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$node --output=${LOG_DIR}/${node}_router_${router_idx}.out python /scripts/worker_setup.py --worker_type sglang-router --worker_idx ${router_idx} --prefill-ips ${PREFILL_IPS_STR} --decode-ips ${DECODE_IPS_STR} --router-port ${ROUTER_INTERNAL_PORT} ${WORKER_ARGS}"
            echo "$cmd"
            $cmd &
            router_idx=$((router_idx + 1))
            router_node_idx=$((router_node_idx + nodes_per_router))
        fi
    done
fi

TOTAL_ROUTERS=$((1 + ADDITIONAL_ROUTERS))
echo "Frontend available at: http://${NGINX_NODE}:8000 (nginx load balancing ${TOTAL_ROUTERS} sglang-routers)"
{% endraw %}
{% else %}
{% raw %}
# Single router architecture - no nginx, router directly on node 0 port 8000
ROUTER_NODE=${nodes[0]}
echo "Launching single sglang-router on ${ROUTER_NODE} (port 8000)"
cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$ROUTER_NODE --output=${LOG_DIR}/${ROUTER_NODE}_router.out python /scripts/worker_setup.py --worker_type sglang-router --worker_idx 0 --prefill-ips ${PREFILL_IPS_STR} --decode-ips ${DECODE_IPS_STR} --router-port 8000 ${WORKER_ARGS}"
echo "$cmd"
$cmd &

echo "Frontend available at: http://${ROUTER_NODE}:8000"
{% endraw %}
{% endif %}
{% endif %}
{% raw %}

echo ""
echo "Make sure to cancel the job at the end:"
echo "scancel $SLURM_JOB_ID"

# Instead of waiting for all tasks to complete, wait for benchmark to complete and then exit.

{% endraw %}

BENCHMARK_TYPE={{ benchmark_type }}
BENCHMARK_ARGS="{{ benchmark_arg }}"
USE_SGLANG_ROUTER={{ "true" if use_sglang_router else "false" }}

{% if do_benchmark %}
{% raw %}
srun --nodes=1 --ntasks=1 $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${nodes[0]} --output=${LOG_DIR}/benchmark.out --overlap bash /scripts/benchmarks/${BENCHMARK_TYPE}/bench.sh $PREFILL_WORKERS $DECODE_WORKERS $PREFILL_GPUS $DECODE_GPUS ${BENCHMARK_ARGS} ${USE_SGLANG_ROUTER} &
{% endraw %}
{% endif %}

{% if profiler != 'none' %}
{% raw %}
# Torch/NSYS profiling mode: run a single orchestrator that profiles all prefill and decode workers.
echo "Starting unified profiler..."

# Collect leader IPs for prefill and decode workers
PREFILL_LEADER_IPS=()
for idx in "${prefill_leaders[@]}"; do
    node_name=${nodes[$idx]}
    ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    PREFILL_LEADER_IPS+=("$ip")
done

DECODE_LEADER_IPS=()
for idx in "${decode_leaders[@]}"; do
    node_name=${nodes[$idx]}
    ip=$(get_node_ip "$node_name" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    DECODE_LEADER_IPS+=("$ip")
done

PREFILL_LEADER_IPS_STR=$(IFS=,; echo "${PREFILL_LEADER_IPS[*]}")
DECODE_LEADER_IPS_STR=$(IFS=,; echo "${DECODE_LEADER_IPS[*]}")

# Use the first prefill leader as the orchestrator node
PROFILE_ORCHESTRATOR_NODE=${nodes[${prefill_leaders[0]}]}
echo "Unified profiling will run on orchestrator node: $PROFILE_ORCHESTRATOR_NODE"

# Run a single profiling orchestrator that coordinates profiling across all leaders
srun --nodes=1 --ntasks=1 $ENROOT_ARGS --jobid $SLURM_JOB_ID -w $PROFILE_ORCHESTRATOR_NODE \
    --output=${LOG_DIR}/profile_all.out --overlap \
    bash -c "PROFILING_MODE=prefill PROFILE_PREFILL_IPS=${PREFILL_LEADER_IPS_STR} PROFILE_DECODE_IPS=${DECODE_LEADER_IPS_STR} {% endraw %}{% if profiler == 'torch' %}SGLANG_TORCH_PROFILER_DIR=/logs/profiles {% endif %}{{ prefill_profile_env }}{% raw %} /scripts/profiling/profile.sh $PREFILL_WORKERS $DECODE_WORKERS $PREFILL_GPUS $DECODE_GPUS $TOTAL_GPUS" &
{% endraw %}
{% endif %}

{% if profiler != 'none' %}
{% raw %}
# Wait for all profiling scripts to complete (both prefill and decode)
echo "Waiting for all profiling scripts to complete..."
wait
exit_code=$?
echo "All profiling scripts finished at $(date) with exit code ${exit_code}"
exit $exit_code
{% endraw %}
{% else %}
{% raw %}
# Wait for first task (benchmark) to complete
wait -n
first_exit_code=$?
echo "Script finished at $(date) with exit code ${first_exit_code}"
exit $first_exit_code
{% endraw %}
{% endif %}
