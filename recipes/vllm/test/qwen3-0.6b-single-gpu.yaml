# Qwen3-0.6B Single GPU Test Config
# Minimal configuration for quick iteration and testing
# Model: Qwen/Qwen3-0.6B (~1.2GB, loads quickly)

name: "qwen3-0.6b-vllm-test"

slurm:
  time_limit: "00:30:00"  # 30 minutes

model:
  # Use hf: prefix to download from HuggingFace at runtime
  path: "hf:Qwen/Qwen3-0.6B"
  container: "/lustre/fsw/coreai_ai-foundation-models_dev/aflowers/containers/nvidia+ai-dynamo+vllm-runtime+0.8.0.sqsh"
  precision: "fp16"

resources:
  gpu_type: "h100"
  gpus_per_node: 8
  # Single aggregated worker on 1 GPU
  agg_nodes: 1
  agg_workers: 1
  gpus_per_agg: 1

frontend:
  type: dynamo
  enable_multiple_frontends: false
  args:
    router-reset-states: true

backend:
  type: vllm
  connector: nixl

  aggregated_environment:
    DYN_HEALTH_CHECK_ENABLED: "false"
    PYTHONUNBUFFERED: "1"

  vllm_config:
    aggregated:
      served-model-name: "Qwen/Qwen3-0.6B"
      tensor-parallel-size: 1
      gpu-memory-utilization: 0.90
      disable-log-requests: true
      max-model-len: 4096

benchmark:
  type: "sa-bench"
  isl: 1024
  osl: 1024
  concurrencies: "1x5x10"
