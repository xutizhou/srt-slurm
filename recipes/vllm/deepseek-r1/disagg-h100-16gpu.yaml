# DeepSeek-R1 Disaggregated with vLLM (H100 16-GPU)
# Based on dynamo exemplar: recipes/deepseek-r1/vllm/disagg/deploy_hopper_16gpu.yaml
#
# H100-optimized version with reduced chunk size for 80GB HBM (vs 141GB on H200)
#
# DP+EP (Data Parallel + Expert Parallel) configuration:
# - Each GPU runs its own vLLM process (not tensor parallel across GPUs)
# - Workers communicate via NCCL for expert routing
# - 1 prefill endpoint × 16 GPUs (2 nodes) = 16 prefill processes
# - 1 decode endpoint × 16 GPUs (2 nodes) = 16 decode processes
# - Total: 32 GPUs across 4 nodes

name: "deepseek-r1-vllm-disagg-h100-16gpu"

model:
  path: "deepseek-r1-fp8"
  container: "vllm-0.8.0"
  precision: "fp8"

resources:
  gpu_type: "h100"
  gpus_per_node: 8
  # Multi-node disaggregated with expert parallel:
  # 1 prefill endpoint using 16 GPUs (2 nodes, DP16) → 16 processes
  # 1 decode endpoint using 16 GPUs (2 nodes, DP16) → 16 processes
  # Total: 32 GPUs across 4 nodes
  prefill_nodes: 2
  decode_nodes: 2
  prefill_workers: 1
  decode_workers: 1
  gpus_per_prefill: 16
  gpus_per_decode: 16

frontend:
  type: dynamo
  enable_multiple_frontends: false

backend:
  type: vllm
  connector: nixl

  prefill_environment:
    VLLM_USE_DEEP_GEMM: "1"
    VLLM_SKIP_P2P_CHECK: "1"
    VLLM_RANDOMIZE_DP_DUMMY_INPUTS: "1"
    NVIDIA_GDRCOPY: "enabled"
    GLOO_SOCKET_IFNAME: "eth0"
    PYTHONUNBUFFERED: "1"

  decode_environment:
    VLLM_USE_DEEP_GEMM: "1"
    VLLM_MOE_DP_CHUNK_SIZE: "192"  # Reduced from 384 for H100 80GB (vs H200 141GB)
    VLLM_SKIP_P2P_CHECK: "1"
    VLLM_RANDOMIZE_DP_DUMMY_INPUTS: "1"
    NVIDIA_GDRCOPY: "enabled"
    GLOO_SOCKET_IFNAME: "eth0"
    PYTHONUNBUFFERED: "1"

  vllm_config:
    prefill:
      served-model-name: "deepseek-ai/DeepSeek-R1"
      all2all-backend: "deepep_high_throughput"
      data-parallel-hybrid-lb: true
      tensor-parallel-size: 1
      data-parallel-size: 16
      data-parallel-rpc-port: 13345
      enable-expert-parallel: true
      max-model-len: 16384
      enable-dbo: true
      dbo-decode-token-threshold: 32
      async-scheduling: true
      enable-eplb: true
      eplb-config: '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}'
      max-num-seqs: 512

    decode:
      served-model-name: "deepseek-ai/DeepSeek-R1"
      all2all-backend: "deepep_low_latency"
      data-parallel-hybrid-lb: true
      tensor-parallel-size: 1
      data-parallel-size: 16
      data-parallel-rpc-port: 13345
      enable-expert-parallel: true
      max-model-len: 16384
      enable-dbo: true
      dbo-decode-token-threshold: 32
      async-scheduling: true
      enable-eplb: true
      eplb-config: '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}'
      max-num-seqs: 512
      compilation-config: '{"pass_config":{"enable_fusion":true,"enable_attn_fusion":true,"enable_noop":true},"custom_ops":["+rms_norm"],"cudagraph_mode":"FULL_DECODE_ONLY"}'

benchmark:
  type: "mooncake-router"
  mooncake_workload: "conversation"
  ttft_threshold_ms: 5000
  itl_threshold_ms: 50
